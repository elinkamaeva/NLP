{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8f3377",
   "metadata": {},
   "source": [
    "## Создание корпуса и разметка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8d18e",
   "metadata": {},
   "source": [
    "Для морфологической разметки я собрала небольшой корпус из 230 словоупотреблений на основе предложений из НКРЯ. После чего я сделала его частеречную разметку, предварительно создав csv-файл с токенами и номерами предложений, к которым они относятся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410119d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf79de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('minicorpus.txt', 'r', encoding='utf-8') as txt_file:\n",
    "     lines = txt_file.readlines()\n",
    "extra = '―«»'\n",
    "tokens = []\n",
    "for line in lines:\n",
    "    line_tokens = word_tokenize(line)\n",
    "    line_tokens = [token for token in line_tokens \n",
    "                   if token[0] not in punctuation and token[0] not in extra and not token.isnumeric()]\n",
    "    line_tokens[0] = line_tokens[0].lower()\n",
    "    tokens.append(line_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea062450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['нежданный',\n",
       "  'снег',\n",
       "  'шуршал',\n",
       "  'всю',\n",
       "  'ночь',\n",
       "  'под',\n",
       "  'утро',\n",
       "  'стих',\n",
       "  'и',\n",
       "  'тут',\n",
       "  'же',\n",
       "  'его',\n",
       "  'влажную',\n",
       "  'шкуру',\n",
       "  'схватило',\n",
       "  'морозцем'],\n",
       " ['ты',\n",
       "  'вечности',\n",
       "  'заложник',\n",
       "  'у',\n",
       "  'времени',\n",
       "  'в',\n",
       "  'плену',\n",
       "  'сколько',\n",
       "  'раз',\n",
       "  'вспоминая',\n",
       "  'этот',\n",
       "  'стих',\n",
       "  'я',\n",
       "  'сбивался',\n",
       "  'забывал',\n",
       "  'где',\n",
       "  'тут',\n",
       "  'время',\n",
       "  'где',\n",
       "  'вечность']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16860f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tokens.csv', 'w', encoding='utf-8') as csv_file:\n",
    "#     csvwriter = csv.writer(csv_file)\n",
    "#     csvwriter.writerow(['token', 'sentence', 'pos'])\n",
    "#     for i in range(len(tokens)):\n",
    "#         for token in tokens[i]:\n",
    "#             csvwriter.writerow([token, i+1, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af127dfd",
   "metadata": {},
   "source": [
    "Приведу примеры предложений, которые попали в корпус:  \n",
    "_(1) Нежданный снег шуршал всю ночь, под утро стих, и тут же его влажную шкуру схватило морозцем.  \n",
    "(2) \"Ты ― вечности заложник, у времени в плену\" ― сколько раз, вспоминая этот стих, я сбивался, забывал, где тут время, где вечность.  \n",
    "(3) В принципе, можно было бы продолжать и дальше, но, думается, это тот случай, когда и так все ясно.  \n",
    "(4) Американская гостья из Риги, возможно, одна из немногих иностранных лидеров, кто ясно представляет себе, когда «начнётся развитие событий вокруг Ирака»._  \n",
    "  \n",
    "В первых двух предложениях мы видим омоним \"стих\", который в __предложении 1__ является глаголом, а в __предложении 2__ существительным. И в __предложениях 3 и 4__ слово \"ясно\" представлено разными частями речи. В одном оно является прилагательном, а в другом — наречием."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3eb29",
   "metadata": {},
   "source": [
    "Мой тегсет выглядит так: NOUN, VERB, ADJ, ADV, PREP, CONJ, PART, PRED  \n",
    "Я выбрала такой набор тегов, потому что он, как мне кажется, наиболее удобен при расхождениях в определении и обозначении частей речи таких слов как _весь_, _вспоминая_, _Анна_. Перечисленные слова я размечала как ADJ, VERB и NOUN соотвественно, не уточняя, что первое — местоименное прилагательное, второе — деепричастие, а третье — имя собственное."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527652f",
   "metadata": {},
   "source": [
    "## Парсеры"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4856129",
   "metadata": {},
   "source": [
    "Дальше я \"прогнала\" свой корпус через три парсера: __mystem__, __pymorphy__ и __spacy__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "293a4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e44ca",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfec88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "with open('spacy.csv', 'w', encoding='utf-8') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow(['token', 'sentence', 'pos'])\n",
    "    for n in range(len(lines)):\n",
    "        sent = nlp(lines[n])\n",
    "        for i in range(len(sent)):\n",
    "            token = sent[i].text\n",
    "            if sent[i].pos_ not in ['PUNCT', 'SPACE'] and not token.isnumeric():\n",
    "                if i == 0:\n",
    "                    token = sent[i].text.lower()\n",
    "                csvwriter.writerow([token, n+1, sent[i].pos_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e5b9a",
   "metadata": {},
   "source": [
    "### Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49e934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f41cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mystem1.csv', 'w', encoding='utf-8') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow(['token', 'sentence', 'pos'])\n",
    "    for n in range(len(lines)):\n",
    "        ana = m.analyze(lines[n])\n",
    "        for i in range(len(ana)):\n",
    "            if 'analysis' in ana[i]:\n",
    "                token = ana[i]['text']\n",
    "                if i == 0:\n",
    "                    token = token.lower()\n",
    "                gr = ana[i]['analysis'][0]['gr']\n",
    "                pos = gr.split('=')[0].split(',')[0]\n",
    "                csvwriter.writerow([token, n+1, pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bca618",
   "metadata": {},
   "source": [
    "### Pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0245f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98abc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pymorphy1.csv', 'w', encoding='utf-8') as csv_file:\n",
    "    csvwriter = csv.writer(csv_file)\n",
    "    csvwriter.writerow(['token', 'sentence', 'pos'])\n",
    "    for n in range(len(tokens)):\n",
    "        for token in tokens[n]:\n",
    "            first = morph.parse(token)[0]  # будем выбирать первый разбор токена\n",
    "            pos = first.tag.POS\n",
    "            csvwriter.writerow([token, n+1, pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e42530",
   "metadata": {},
   "source": [
    "## Оценим accuracy теггеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eee3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(csvfile):\n",
    "    df = pd.read_csv(csvfile)\n",
    "    df['pos'] = df['pos'].replace(['DET', 'ADJF', 'ADJS', 'A', 'COMP', 'APRO'], 'ADJ')\n",
    "    df['pos'] = df['pos'].replace(['INFN', 'PRTF', 'PRTS', 'GRND', 'V', 'AUX'], 'VERB')\n",
    "    df['pos'] = df['pos'].replace(['CCONJ', 'SCONJ'], 'CONJ')\n",
    "    df['pos'] = df['pos'].replace(['ADP', 'PR'], 'PREP')\n",
    "    df['pos'] = df['pos'].replace(['NPRO', 'SPRO'], 'PRON')\n",
    "    df['pos'] = df['pos'].replace(['ADVB', 'ADVPRO'], 'ADV')\n",
    "    df['pos'] = df['pos'].replace(['PROPN', 'S'], 'NOUN')\n",
    "    df['pos'] = df['pos'].replace('PRTCL', 'PART')\n",
    "    df.to_csv(csvfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "731bbb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize('spacy.csv')\n",
    "standardize('mystem.csv')\n",
    "standardize('pymorphy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1700c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.read_csv('tokens.csv')\n",
    "y = list(my_df['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "575f8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "165e5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_accuracy(y, parser_file):\n",
    "    df = pd.read_csv(parser_file)\n",
    "    y_pred = list(df['pos'])\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cccd974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У Spacy accuracy=0.9118942731277533\n",
      "У Mystem accuracy=0.960352422907489\n",
      "У Pymorphy accuracy=0.8722466960352423\n"
     ]
    }
   ],
   "source": [
    "spacy_acc = count_accuracy(y, 'spacy.csv')\n",
    "mystem_acc = count_accuracy(y, 'mystem.csv')\n",
    "pymorphy_acc = count_accuracy(y, 'pymorphy.csv')\n",
    "print(f'У Spacy accuracy={spacy_acc}')\n",
    "print(f'У Mystem accuracy={mystem_acc}')\n",
    "print(f'У Pymorphy accuracy={pymorphy_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd14ee4",
   "metadata": {},
   "source": [
    "## Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdbb5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(word):\n",
    "    pos = None\n",
    "    if 'analysis' in word:\n",
    "        token = word['text']\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c14fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_chunker(lines):\n",
    "    bigrams = []\n",
    "    for n in range(len(lines)):\n",
    "        ana = m.analyze(lines[n])\n",
    "        for i in range(2, len(ana)):\n",
    "            pos1 = pos(ana[i])\n",
    "            if pos1 == 'S':\n",
    "                pos2 = pos(ana[i-2])\n",
    "                if pos2 == 'A' or pos2 == 'V':\n",
    "                    phrase = ana[i-2]['text'] + ' ' + ana[i]['text']\n",
    "                    bigrams.append(phrase.lower())\n",
    "            elif pos1 == 'V' and ana[i-2]['text'] == 'не':\n",
    "                phrase = 'не ' + ana[i]['text']\n",
    "                bigrams.append(phrase.lower())\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1782e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['нежданный снег',\n",
       " 'влажную шкуру',\n",
       " 'схватило морозцем',\n",
       " 'вегетарианская столовая',\n",
       " 'успокаивающий настой',\n",
       " 'столовая ложка',\n",
       " 'общественной жизни',\n",
       " 'бывалый служака',\n",
       " 'не нарушай',\n",
       " 'нарушай дистанции',\n",
       " 'тупые смски',\n",
       " 'показного мнения',\n",
       " 'интересные данные',\n",
       " 'последние годы',\n",
       " 'чёрных дыр',\n",
       " 'звёздной массы',\n",
       " 'не выполнять',\n",
       " 'данные обещания',\n",
       " 'тёплого дома',\n",
       " 'холодное утро',\n",
       " 'не хочет',\n",
       " 'ловить рыбу',\n",
       " 'американская гостья',\n",
       " 'иностранных лидеров',\n",
       " 'начнётся развитие']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_chunker(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186609f",
   "metadata": {},
   "source": [
    "## Применение чанкера\n",
    "Теперь применим наш чанкер к программе из дз 1  \n",
    "_Примечание_. При использовании в программе определения тональности отзыва я его отредактировала, чтобы он не \"выплевывал\" словосочетания, а заменял их последовательность слов с нижним подчеркиванием типа _целый_день_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d51e62e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(word):\n",
    "    pos = None\n",
    "    if 'analysis' in word and word['analysis']:\n",
    "        token = word['text']\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfde0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(lines):\n",
    "    for n in range(len(lines)):\n",
    "        ana = m.analyze(lines[n])\n",
    "        for i in range(2, len(ana)):\n",
    "            pos1 = pos(ana[i])\n",
    "            if pos1 == 'S':\n",
    "                pos2 = pos(ana[i-2])\n",
    "                if pos2 == 'A' or pos2 == 'V':\n",
    "                    old = ana[i-2]['text'] + ' ' + ana[i]['text']\n",
    "                    new = ana[i-2]['text'] + '_' + ana[i]['text']\n",
    "                    lines[n] = lines[n].replace(old, new.lower())\n",
    "            elif pos1 == 'V' and ana[i-2]['text'] == 'не':\n",
    "                old = 'не ' + ana[i]['text']\n",
    "                new = 'не_' + ana[i]['text']\n",
    "                lines[n] = lines[n].replace(old, new.lower())\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc9e31de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D Touch просто восхитительная вещь! Заряд дер...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Отключается при температуре близкой к нулю, не...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>В Apple окончательно решили не заморачиваться,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Постарался наиболее ёмко и коротко описать все...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Достойный телефон. Пользоваться одно удовольст...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating\n",
       "0  3D Touch просто восхитительная вещь! Заряд дер...       5\n",
       "1  Отключается при температуре близкой к нулю, не...       4\n",
       "2  В Apple окончательно решили не заморачиваться,...       3\n",
       "3  Постарался наиболее ёмко и коротко описать все...       4\n",
       "4  Достойный телефон. Пользоваться одно удовольст...       5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c17d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df = df[df['Rating'] <= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1d9a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews = list(df['Review'][df['Rating'] < 4][:55])\n",
    "neg_train = neg_reviews[:50]\n",
    "neg_test = neg_reviews[50:]\n",
    "\n",
    "pos_reviews = list(df['Review'][df['Rating'] >= 4][:55])\n",
    "pos_train = pos_reviews[:50]\n",
    "pos_test = pos_reviews[50:]\n",
    "\n",
    "X_train = pos_train + neg_train\n",
    "X_test = pos_test + neg_test\n",
    "y_train = [1] * 50 + [0] * 50\n",
    "y_test = [1] * 5 + [0] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f7a7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2eb87b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('russian')\n",
    "\n",
    "def lemmatize(reviews_list):\n",
    "    lemmas_w_noise = [morph.parse(word)[0].normal_form for review in reviews_list\n",
    "                      for sent in nltk.sent_tokenize(review)\n",
    "                      for word in nltk.word_tokenize(sent)]\n",
    "    text = ' '.join(lemmas_w_noise)\n",
    "    lines = nltk.sent_tokenize(text)\n",
    "    sentences = chunker(lines)\n",
    "    lemmas = [lemma for sent in sentences for lemma in nltk.word_tokenize(sent)\n",
    "              if lemma not in punctuation and lemma not in sw]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d94e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lemmas):\n",
    "    counted = Counter(lemmas)\n",
    "    new_lemmas = set({k:v for k,v in counted.items() if v > 2 })\n",
    "    return new_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9814d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_lemmas = lemmatize(neg_train)\n",
    "neg_lemmas = most_common(neg_lemmas)\n",
    "\n",
    "pos_lemmas = lemmatize(pos_train)\n",
    "pos_lemmas = most_common(pos_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "22931ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemmas.difference_update(neg_lemmas)\n",
    "neg_lemmas.difference_update(pos_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7a52cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(review):\n",
    "    lemmas_w_noise = [morph.parse(word)[0].normal_form \n",
    "                      for sent in nltk.sent_tokenize(review)\n",
    "                      for word in nltk.word_tokenize(sent)]\n",
    "    lemmas = [lemma for lemma in lemmas_w_noise\n",
    "              if lemma not in punctuation and lemma not in sw]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d122616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_type(text):\n",
    "    lemmas = set(lemmatize_text(text))\n",
    "    if len(lemmas.intersection(pos_lemmas)) >= len(lemmas.intersection(neg_lemmas)):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84b715e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tr = [review_type(r) for r in X_train]\n",
    "y_pred_te = [review_type(r) for r in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "126c51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = accuracy_score(y_train, y_pred_tr)\n",
    "test_acc = accuracy_score(y_test, y_pred_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e512991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "print(train_acc)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083450a",
   "metadata": {},
   "source": [
    "Таким образом, мы видим, что наш чанкер улучшил результат только на трейне и то на 0.01, а на тестовых данных качество не изменилось. Я думаю, так вышло потому, что данных очень мало"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
